{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This code is modified version of code from https://www.kaggle.com/pestipeti/pytorch-starter-fasterrcnn-train","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport cv2\nimport os\nimport re\nimport time\nimport copy\nfrom PIL import Image\n\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\n\nimport torch\nimport torchvision\nfrom sklearn.utils import shuffle\n\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection import FasterRCNN\nfrom torchvision.models.detection.rpn import AnchorGenerator\n\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.utils.data.sampler import SequentialSampler\n\nfrom matplotlib import pyplot as plt\nfrom numba import jit\n\nDIR_INPUT = '../input/adl-a1-cropping/cropping_object_detection_data'\nDIR_TRAIN = f'{DIR_INPUT}/train'\nDIR_TEST = f'{DIR_INPUT}/test'","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2021-10-11T09:07:39.983909Z","iopub.execute_input":"2021-10-11T09:07:39.984250Z","iopub.status.idle":"2021-10-11T09:07:39.992350Z","shell.execute_reply.started":"2021-10-11T09:07:39.984200Z","shell.execute_reply":"2021-10-11T09:07:39.991215Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(f'{DIR_INPUT}/train_data.csv')\ntrain_df.shape","metadata":{"execution":{"iopub.status.busy":"2021-10-11T09:07:41.011864Z","iopub.execute_input":"2021-10-11T09:07:41.012198Z","iopub.status.idle":"2021-10-11T09:07:41.029046Z","shell.execute_reply.started":"2021-10-11T09:07:41.012147Z","shell.execute_reply":"2021-10-11T09:07:41.027970Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-11T09:07:41.388903Z","iopub.execute_input":"2021-10-11T09:07:41.389220Z","iopub.status.idle":"2021-10-11T09:07:41.400747Z","shell.execute_reply.started":"2021-10-11T09:07:41.389168Z","shell.execute_reply":"2021-10-11T09:07:41.399798Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"train_df['x'] = train_df['x'].astype(np.float)\ntrain_df['y'] = train_df['y'].astype(np.float)\ntrain_df['w'] = train_df['w'].astype(np.float)\ntrain_df['h'] = train_df['h'].astype(np.float)","metadata":{"execution":{"iopub.status.busy":"2021-10-11T09:07:42.644922Z","iopub.execute_input":"2021-10-11T09:07:42.645269Z","iopub.status.idle":"2021-10-11T09:07:42.655720Z","shell.execute_reply.started":"2021-10-11T09:07:42.645216Z","shell.execute_reply":"2021-10-11T09:07:42.652912Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"train_df = shuffle(train_df, random_state=1411)","metadata":{"execution":{"iopub.status.busy":"2021-10-11T09:07:44.480988Z","iopub.execute_input":"2021-10-11T09:07:44.481321Z","iopub.status.idle":"2021-10-11T09:07:44.488617Z","shell.execute_reply.started":"2021-10-11T09:07:44.481270Z","shell.execute_reply":"2021-10-11T09:07:44.487341Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"image_ids = train_df['image_id'].unique()\nvalid_ids = image_ids[-486:]\ntrain_ids = image_ids[:-486]","metadata":{"execution":{"iopub.status.busy":"2021-10-11T09:07:44.807971Z","iopub.execute_input":"2021-10-11T09:07:44.808261Z","iopub.status.idle":"2021-10-11T09:07:44.815606Z","shell.execute_reply.started":"2021-10-11T09:07:44.808215Z","shell.execute_reply":"2021-10-11T09:07:44.814615Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"valid_df = train_df[train_df['image_id'].isin(valid_ids)]\ntrain_df = train_df[train_df['image_id'].isin(train_ids)]","metadata":{"execution":{"iopub.status.busy":"2021-10-11T09:07:45.899557Z","iopub.execute_input":"2021-10-11T09:07:45.899915Z","iopub.status.idle":"2021-10-11T09:07:45.908301Z","shell.execute_reply.started":"2021-10-11T09:07:45.899861Z","shell.execute_reply":"2021-10-11T09:07:45.907002Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"valid_df.shape, train_df.shape","metadata":{"execution":{"iopub.status.busy":"2021-10-11T09:07:46.513788Z","iopub.execute_input":"2021-10-11T09:07:46.514143Z","iopub.status.idle":"2021-10-11T09:07:46.520887Z","shell.execute_reply.started":"2021-10-11T09:07:46.514090Z","shell.execute_reply":"2021-10-11T09:07:46.519808Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"class DetectionDataset(Dataset):\n\n    def __init__(self, dataframe, image_dir, transforms=None):\n        super().__init__()\n\n        self.image_ids = dataframe['image_id'].unique()\n        self.df = dataframe\n        self.image_dir = image_dir\n        self.transforms = transforms\n\n    def __getitem__(self, index: int):\n\n        image_id = self.image_ids[index]\n        records = self.df[self.df['image_id'] == image_id]\n        image = cv2.imread(f'{self.image_dir}/{image_id}.jpg', cv2.IMREAD_COLOR)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        image /= 255.0\n\n        temp_boxes = records[['x', 'y', 'w', 'h']].values\n        boxes = temp_boxes.copy()\n        boxes[:, 0] = temp_boxes[:, 0] - temp_boxes[:, 2]/2\n        boxes[:, 1] = temp_boxes[:, 1] - temp_boxes[:, 3]/2\n        boxes[:, 2] = temp_boxes[:, 0] + temp_boxes[:, 2]/2\n        boxes[:, 3] = temp_boxes[:, 1] + temp_boxes[:, 3]/2\n        \n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n        area = torch.as_tensor(area, dtype=torch.float32)\n\n        # there is only one class\n        labels = torch.ones((records.shape[0],), dtype=torch.int64)\n        \n        # suppose all instances are not crowd\n        iscrowd = torch.zeros((records.shape[0],), dtype=torch.int64)\n        \n        target = {}\n        target['boxes'] = boxes\n        target['labels'] = labels\n        target['image_id'] = torch.tensor([index])\n        target['area'] = area\n        target['iscrowd'] = iscrowd\n\n        if self.transforms:\n            sample = {\n                'image': image,\n                'bboxes': target['boxes'],\n                'labels': labels\n            }\n            sample = self.transforms(**sample)\n            image = sample['image']\n            \n            target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n\n        return image, target, image_id\n\n    def __len__(self) -> int:\n        return self.image_ids.shape[0]","metadata":{"execution":{"iopub.status.busy":"2021-10-11T09:07:46.655271Z","iopub.execute_input":"2021-10-11T09:07:46.655629Z","iopub.status.idle":"2021-10-11T09:07:46.673816Z","shell.execute_reply.started":"2021-10-11T09:07:46.655565Z","shell.execute_reply":"2021-10-11T09:07:46.672686Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# Albumentations\ndef get_train_transform():\n    return A.Compose([\n        A.Resize(800,600),\n        A.HorizontalFlip(0.3),\n        A.RandomBrightnessContrast(p=0.1),\n        A.Rotate(limit = 15, p= .4),\n        ToTensorV2(p=1.0)\n    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n\ndef get_valid_transform():\n    return A.Compose([\n        ToTensorV2(p=1.0)\n    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n","metadata":{"execution":{"iopub.status.busy":"2021-10-11T09:07:49.705198Z","iopub.execute_input":"2021-10-11T09:07:49.705566Z","iopub.status.idle":"2021-10-11T09:07:49.713198Z","shell.execute_reply.started":"2021-10-11T09:07:49.705511Z","shell.execute_reply":"2021-10-11T09:07:49.712127Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"# Create the model","metadata":{}},{"cell_type":"code","source":"# load a model; pre-trained on COCO\nmodel = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)","metadata":{"execution":{"iopub.status.busy":"2021-10-11T09:07:52.078554Z","iopub.execute_input":"2021-10-11T09:07:52.078897Z","iopub.status.idle":"2021-10-11T09:08:01.471236Z","shell.execute_reply.started":"2021-10-11T09:07:52.078837Z","shell.execute_reply":"2021-10-11T09:08:01.470302Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"num_classes = 2  # 1 class (head) + background\n\n# get number of input features for the classifier\nin_features = model.roi_heads.box_predictor.cls_score.in_features\n\n# replace the pre-trained head with a new one\nmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)","metadata":{"execution":{"iopub.status.busy":"2021-10-11T09:08:01.473025Z","iopub.execute_input":"2021-10-11T09:08:01.473552Z","iopub.status.idle":"2021-10-11T09:08:01.479884Z","shell.execute_reply.started":"2021-10-11T09:08:01.473497Z","shell.execute_reply":"2021-10-11T09:08:01.479080Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"class Averager:\n    def __init__(self):\n        self.current_total = 0.0\n        self.iterations = 0.0\n\n    def send(self, value):\n        self.current_total += value\n        self.iterations += 1\n\n    @property\n    def value(self):\n        if self.iterations == 0:\n            return 0\n        else:\n            return 1.0 * self.current_total / self.iterations\n\n    def reset(self):\n        self.current_total = 0.0\n        self.iterations = 0.0\n","metadata":{"execution":{"iopub.status.busy":"2021-10-11T09:08:01.481610Z","iopub.execute_input":"2021-10-11T09:08:01.482199Z","iopub.status.idle":"2021-10-11T09:08:01.490778Z","shell.execute_reply.started":"2021-10-11T09:08:01.482033Z","shell.execute_reply":"2021-10-11T09:08:01.489502Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"def collate_fn(batch):\n    return tuple(zip(*batch))\n\ntrain_dataset = DetectionDataset(train_df, DIR_TRAIN, get_train_transform())\nvalid_dataset = DetectionDataset(valid_df, DIR_TRAIN, get_valid_transform())\n\n\n# split the dataset in train and test set\nindices = torch.randperm(len(train_dataset)).tolist()\n\ntrain_data_loader = DataLoader(\n    train_dataset,\n    batch_size=12,\n    shuffle=True,\n    num_workers=4,\n    collate_fn=collate_fn\n)\n\nvalid_data_loader = DataLoader(\n    valid_dataset,\n    batch_size=16,\n    shuffle=False,\n    num_workers=4,\n    collate_fn=collate_fn\n)\ndataloaders = {\n    \"train\": train_data_loader, \n    \"val\": valid_data_loader,\n}","metadata":{"execution":{"iopub.status.busy":"2021-10-11T09:08:01.492269Z","iopub.execute_input":"2021-10-11T09:08:01.493016Z","iopub.status.idle":"2021-10-11T09:08:01.510681Z","shell.execute_reply.started":"2021-10-11T09:08:01.492676Z","shell.execute_reply":"2021-10-11T09:08:01.509964Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\ndevice","metadata":{"execution":{"iopub.status.busy":"2021-10-11T09:08:13.907232Z","iopub.execute_input":"2021-10-11T09:08:13.907805Z","iopub.status.idle":"2021-10-11T09:08:13.915073Z","shell.execute_reply.started":"2021-10-11T09:08:13.907741Z","shell.execute_reply":"2021-10-11T09:08:13.913806Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"# Sample","metadata":{}},{"cell_type":"code","source":"sample = train_dataset[0]","metadata":{"execution":{"iopub.status.busy":"2021-10-11T09:08:15.999472Z","iopub.execute_input":"2021-10-11T09:08:15.999810Z","iopub.status.idle":"2021-10-11T09:08:16.062536Z","shell.execute_reply.started":"2021-10-11T09:08:15.999758Z","shell.execute_reply":"2021-10-11T09:08:16.061746Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"sample[0].shape","metadata":{"execution":{"iopub.status.busy":"2021-10-11T09:08:17.329958Z","iopub.execute_input":"2021-10-11T09:08:17.330312Z","iopub.status.idle":"2021-10-11T09:08:17.339228Z","shell.execute_reply.started":"2021-10-11T09:08:17.330258Z","shell.execute_reply":"2021-10-11T09:08:17.338293Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"sampleiter = iter(train_data_loader)","metadata":{"execution":{"iopub.status.busy":"2021-10-11T09:08:18.660627Z","iopub.execute_input":"2021-10-11T09:08:18.660990Z","iopub.status.idle":"2021-10-11T09:08:18.729914Z","shell.execute_reply.started":"2021-10-11T09:08:18.660937Z","shell.execute_reply":"2021-10-11T09:08:18.725909Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"images, targets, image_ids = next(sampleiter)","metadata":{"execution":{"iopub.status.busy":"2021-10-11T09:08:19.727504Z","iopub.execute_input":"2021-10-11T09:08:19.728312Z","iopub.status.idle":"2021-10-11T09:08:20.003616Z","shell.execute_reply.started":"2021-10-11T09:08:19.728240Z","shell.execute_reply":"2021-10-11T09:08:20.002511Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"images, targets, image_ids = next(sampleiter)\nimages = list(image.to(device) for image in images)\ntargets = [{k: v.to(device) for k, v in t.items()} for t in targets]","metadata":{"execution":{"iopub.status.busy":"2021-10-11T09:08:21.163375Z","iopub.execute_input":"2021-10-11T09:08:21.163738Z","iopub.status.idle":"2021-10-11T09:08:24.586373Z","shell.execute_reply.started":"2021-10-11T09:08:21.163688Z","shell.execute_reply":"2021-10-11T09:08:24.585581Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"boxes = targets[0]['boxes'].cpu().numpy().astype(np.int32)\nsample = images[0].permute(1,2,0).cpu().numpy()","metadata":{"execution":{"iopub.status.busy":"2021-10-11T09:08:24.674879Z","iopub.execute_input":"2021-10-11T09:08:24.675172Z","iopub.status.idle":"2021-10-11T09:08:24.684049Z","shell.execute_reply.started":"2021-10-11T09:08:24.675126Z","shell.execute_reply":"2021-10-11T09:08:24.683263Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n\nfor box in boxes:\n    cv2.rectangle(sample,\n                  (box[0], box[1]),\n                  (box[2], box[3]),\n                  (220, 0, 0), 3)\n    \nax.set_axis_off()\nax.imshow(sample)","metadata":{"execution":{"iopub.status.busy":"2021-10-11T09:08:25.960062Z","iopub.execute_input":"2021-10-11T09:08:25.960463Z","iopub.status.idle":"2021-10-11T09:08:26.303293Z","shell.execute_reply.started":"2021-10-11T09:08:25.960386Z","shell.execute_reply":"2021-10-11T09:08:26.302494Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"The below code for IoU calculation and precision calculation is taken from https://www.kaggle.com/havinath/object-detection-using-pytorch-training#Obect-detection-using-PyTorch---Training. ","metadata":{}},{"cell_type":"code","source":"@jit(nopython=True)\ndef calculate_iou(gt, pr, form='pascal_voc') -> float:\n    \"\"\"Calculates the Intersection over Union.\n\n    Args:\n        gt: (np.ndarray[Union[int, float]]) coordinates of the ground-truth box\n        pr: (np.ndarray[Union[int, float]]) coordinates of the prdected box\n        form: (str) gt/pred coordinates format\n            - pascal_voc: [xmin, ymin, xmax, ymax]\n            - coco: [xmin, ymin, w, h]\n    Returns:\n        (float) Intersection over union (0.0 <= iou <= 1.0)\n    \"\"\"\n    if form == 'coco':\n        gt = gt.copy()\n        pr = pr.copy()\n\n        gt[2] = gt[0] + gt[2]\n        gt[3] = gt[1] + gt[3]\n        pr[2] = pr[0] + pr[2]\n        pr[3] = pr[1] + pr[3]\n\n    # Calculate overlap area\n    dx = min(gt[2], pr[2]) - max(gt[0], pr[0]) + 1\n    \n    if dx < 0:\n        return 0.0\n    \n    dy = min(gt[3], pr[3]) - max(gt[1], pr[1]) + 1\n\n    if dy < 0:\n        return 0.0\n\n    overlap_area = dx * dy\n\n    # Calculate union area\n    union_area = (\n            (gt[2] - gt[0] + 1) * (gt[3] - gt[1] + 1) +\n            (pr[2] - pr[0] + 1) * (pr[3] - pr[1] + 1) -\n            overlap_area\n    )\n\n    return overlap_area / union_area\n\n@jit(nopython=True)\ndef find_best_match(gts, pred, pred_idx, threshold = 0.5, form = 'pascal_voc', ious=None) -> int:\n    \"\"\"Returns the index of the 'best match' between the\n    ground-truth boxes and the prediction. The 'best match'\n    is the highest IoU. (0.0 IoUs are ignored).\n\n    Args:\n        gts: (List[List[Union[int, float]]]) Coordinates of the available ground-truth boxes\n        pred: (List[Union[int, float]]) Coordinates of the predicted box\n        pred_idx: (int) Index of the current predicted box\n        threshold: (float) Threshold\n        form: (str) Format of the coordinates\n        ious: (np.ndarray) len(gts) x len(preds) matrix for storing calculated ious.\n\n    Return:\n        (int) Index of the best match GT box (-1 if no match above threshold)\n    \"\"\"\n    best_match_iou = -np.inf\n    best_match_idx = -1\n\n    for gt_idx in range(len(gts)):\n        \n        if gts[gt_idx][0] < 0:\n            # Already matched GT-box\n            continue\n        \n        iou = -1 if ious is None else ious[gt_idx][pred_idx]\n\n        if iou < 0:\n            iou = calculate_iou(gts[gt_idx], pred, form=form)\n            \n            if ious is not None:\n                ious[gt_idx][pred_idx] = iou\n\n        if iou < threshold:\n            continue\n\n        if iou > best_match_iou:\n            best_match_iou = iou\n            best_match_idx = gt_idx\n\n    return best_match_idx\n\n@jit(nopython=True)\ndef calculate_precision(gts, preds, threshold = 0.5, form = 'coco', ious=None) -> float:\n    \"\"\"Calculates precision for GT - prediction pairs at one threshold.\n\n    Args:\n        gts: (List[List[Union[int, float]]]) Coordinates of the available ground-truth boxes\n        preds: (List[List[Union[int, float]]]) Coordinates of the predicted boxes,\n               sorted by confidence value (descending)\n        threshold: (float) Threshold\n        form: (str) Format of the coordinates\n        ious: (np.ndarray) len(gts) x len(preds) matrix for storing calculated ious.\n\n    Return:\n        (float) Precision\n    \"\"\"\n    n = len(preds)\n    tp = 0\n    fp = 0\n    \n    # for pred_idx, pred in enumerate(preds_sorted):\n    for pred_idx in range(n):\n\n        best_match_gt_idx = find_best_match(gts, preds[pred_idx], pred_idx,\n                                            threshold=threshold, form=form, ious=ious)\n\n        if best_match_gt_idx >= 0:\n            # True positive: The predicted box matches a gt box with an IoU above the threshold.\n            tp += 1\n            # Remove the matched GT box\n            gts[best_match_gt_idx] = -1\n\n        else:\n            # No match\n            # False positive: indicates a predicted box had no associated gt box.\n            fp += 1\n\n    # False negative: indicates a gt box had no associated predicted box.\n    fn = (gts.sum(axis=1) > 0).sum()\n\n    return tp / (tp + fp + fn)\n\n\n@jit(nopython=True)\ndef calculate_image_precision(gts, preds, thresholds = (0.5, ), form = 'coco') -> float:\n    \"\"\"Calculates image precision.\n\n    Args:\n        gts: (List[List[Union[int, float]]]) Coordinates of the available ground-truth boxes\n        preds: (List[List[Union[int, float]]]) Coordinates of the predicted boxes,\n               sorted by confidence value (descending)\n        thresholds: (float) Different thresholds\n        form: (str) Format of the coordinates\n\n    Return:\n        (float) Precision\n    \"\"\"\n    n_threshold = len(thresholds)\n    image_precision = 0.0\n    \n    ious = np.ones((len(gts), len(preds))) * -1\n    # ious = None\n\n    for threshold in thresholds:\n        precision_at_threshold = calculate_precision(gts.copy(), preds, threshold=threshold,\n                                                     form=form, ious=ious)\n        image_precision += precision_at_threshold / n_threshold\n\n    return image_precision","metadata":{"execution":{"iopub.status.busy":"2021-10-11T09:09:37.745185Z","iopub.execute_input":"2021-10-11T09:09:37.745603Z","iopub.status.idle":"2021-10-11T09:09:37.772655Z","shell.execute_reply.started":"2021-10-11T09:09:37.745546Z","shell.execute_reply":"2021-10-11T09:09:37.771766Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"iou_thresholds = [x for x in np.arange(0.5, 0.75, 0.1)]\n\ndef get_val_iou(predictions, gt_boxes):\n    validation_image_precisions = []\n    for i in range(len(predictions)):\n        preds = predictions[i][\"boxes\"].data.cpu().numpy().astype(np.int32)\n        scores = predictions[i][\"scores\"].data.cpu().numpy().astype(np.int32)\n        preds_sorted_idx = np.argsort(scores)[::-1]\n        preds_sorted = preds[preds_sorted_idx]\n        gt_box = gt_boxes[i]['boxes'].cpu().numpy().astype(np.int32)\n        image_precision = calculate_image_precision(preds_sorted,\n                                                    gt_box,\n                                                    thresholds=iou_thresholds,\n                                                    form='pascal_voc')\n        validation_image_precisions.append(image_precision)\n    \n    val_iou = np.mean(validation_image_precisions)\n    return val_iou","metadata":{"execution":{"iopub.status.busy":"2021-10-11T09:09:39.034245Z","iopub.execute_input":"2021-10-11T09:09:39.034629Z","iopub.status.idle":"2021-10-11T09:09:39.044965Z","shell.execute_reply.started":"2021-10-11T09:09:39.034573Z","shell.execute_reply":"2021-10-11T09:09:39.043887Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"# Train","metadata":{}},{"cell_type":"code","source":"model.to(device)\nparams = [p for p in model.parameters() if p.requires_grad]\n# optimizer = torch.optim.Adam(params, lr=0.003, weight_decay=0.0005)\noptimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n# lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\nlr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n                                                          mode='max', \n                                                          factor=0.1, \n                                                          patience=3,\n                                                          verbose=True)\nnum_epochs = 25","metadata":{"execution":{"iopub.status.busy":"2021-10-11T09:09:45.308059Z","iopub.execute_input":"2021-10-11T09:09:45.308530Z","iopub.status.idle":"2021-10-11T09:09:45.378145Z","shell.execute_reply.started":"2021-10-11T09:09:45.308340Z","shell.execute_reply":"2021-10-11T09:09:45.377383Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"train_loss_avg =  Averager()\nval_iou_avg = Averager()\n\ntrain_loss_hist = []\nval_iou_hist = []\n\nitr = 0\nbest_iou = 0\nbest_model_wts = copy.deepcopy(model.state_dict())\nbest_epoch_num = -1","metadata":{"execution":{"iopub.status.busy":"2021-10-11T09:09:46.900648Z","iopub.execute_input":"2021-10-11T09:09:46.901272Z","iopub.status.idle":"2021-10-11T09:09:46.939265Z","shell.execute_reply.started":"2021-10-11T09:09:46.901213Z","shell.execute_reply":"2021-10-11T09:09:46.938462Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"for epoch in range(num_epochs):\n    print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n    print('-' * 10)\n    \n    since = time.time()\n    \n    for phase  in  [\"train\", \"val\"]:\n        if phase == \"train\":\n            model.train()\n            train_loss_avg.reset()\n        else:\n            model.eval()\n            val_iou_avg.reset()\n            \n        running_loss = 0\n        running_iou = 0\n        counter = 0\n        for images, targets, image_ids in dataloaders[phase]:\n            if phase == \"train\": \n                images = list(image.to(device) for image in images)\n                targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n                optimizer.zero_grad()\n        \n                loss_dict = model(images, targets)\n                losses = sum(loss for loss in loss_dict.values())\n                losses.backward()\n                optimizer.step()\n                \n                loss_value = losses.item()\n                train_loss_avg.send(loss_value)\n                running_loss += loss_value*len(images)\n                \n                if counter % 50 == 0:\n                    print(f\"{phase} Iteration #{itr} loss: {loss_value}\")\n                itr += 1\n            \n            elif phase == \"val\":\n                images = list(image.to(device) for image in images)\n                targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n                with torch.no_grad():\n                    outputs = model(images)\n                \n                val_iou = get_val_iou(outputs, targets)\n                val_iou_avg.send(val_iou)\n                running_iou += val_iou*len(images)\n                if counter % 50 == 0:\n                    print(f\"{phase} Iteration #{itr} IoU: {val_iou}\")\n                    \n                \n                \n            counter += 1\n            \n        if phase == \"train\":\n            epoch_loss = running_loss/len(dataloaders[phase].dataset)\n            train_loss_hist.append(epoch_loss)\n        else:\n            epoch_iou = running_iou/len(dataloaders[phase].dataset)\n            val_iou_hist.append(epoch_iou)\n            \n            \n        if phase == \"val\" and epoch_iou > best_iou:\n            best_iou  = epoch_iou\n            best_model_wts = copy.deepcopy(model.state_dict())\n            best_epoch_num = epoch\n            \n        if phase == \"val\" and lr_scheduler is not None:\n            lr_scheduler.step(epoch_iou)\n            \n        if phase == \"train\":\n            print(f\"{phase} Epoch #{epoch} loss: {train_loss_avg.value} | Best Val IoU: {best_iou}\")\n        else:\n            print(f\"{phase} Epoch #{epoch} IoU: {val_iou_avg.value} | Best Val IoU: {best_iou}\")\n        \n        if phase == \"val\":\n            name = str(epoch%2)\n            exp_dict = {\n                \"current_model_wts\": copy.deepcopy(model.state_dict()),\n                \"train_loss_hist\": train_loss_hist,\n                \"val_iou_hist\":val_iou_hist,\n                \"current_optimizer_wts\": copy.deepcopy(optimizer.state_dict()), \n                \"current_lr_scheduler_wts\": copy.deepcopy(lr_scheduler.state_dict()),\n                \"current_epoch_num\": epoch,\n                \"best_model_wts\": best_model_wts,\n                \"best_epoch_num\": best_epoch_num\n            }\n            torch.save(exp_dict, f\"exp_dict_{name}_run1.pth\")\n            \n    time_elapsed = time.time() - since\n    print('Epoch complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))","metadata":{"execution":{"iopub.status.busy":"2021-10-10T12:43:13.820474Z","iopub.execute_input":"2021-10-10T12:43:13.82084Z","iopub.status.idle":"2021-10-10T15:05:37.576975Z","shell.execute_reply.started":"2021-10-10T12:43:13.820787Z","shell.execute_reply":"2021-10-10T15:05:37.575703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.load_state_dict(best_model_wts)","metadata":{"execution":{"iopub.status.busy":"2021-10-10T15:07:23.235429Z","iopub.execute_input":"2021-10-10T15:07:23.23572Z","iopub.status.idle":"2021-10-10T15:07:23.264421Z","shell.execute_reply.started":"2021-10-10T15:07:23.235673Z","shell.execute_reply":"2021-10-10T15:07:23.26335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_iter  = iter(valid_data_loader)","metadata":{"execution":{"iopub.status.busy":"2021-10-10T15:07:53.599055Z","iopub.execute_input":"2021-10-10T15:07:53.599405Z","iopub.status.idle":"2021-10-10T15:07:55.042147Z","shell.execute_reply.started":"2021-10-10T15:07:53.599352Z","shell.execute_reply":"2021-10-10T15:07:55.039699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images, targets, image_ids = next(val_iter)\nimages = list(img.to(device) for img in images)\ntargets = [{k: v.to(device) for k, v in t.items()} for t in targets]","metadata":{"execution":{"iopub.status.busy":"2021-10-10T15:09:18.65192Z","iopub.execute_input":"2021-10-10T15:09:18.65226Z","iopub.status.idle":"2021-10-10T15:09:18.743454Z","shell.execute_reply.started":"2021-10-10T15:09:18.652206Z","shell.execute_reply":"2021-10-10T15:09:18.742562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"boxes = targets[0]['boxes'].cpu().numpy().astype(np.int32)\nsample = images[0].permute(1,2,0).cpu().numpy()","metadata":{"execution":{"iopub.status.busy":"2021-10-10T15:09:19.720352Z","iopub.execute_input":"2021-10-10T15:09:19.720689Z","iopub.status.idle":"2021-10-10T15:09:19.727867Z","shell.execute_reply.started":"2021-10-10T15:09:19.720635Z","shell.execute_reply":"2021-10-10T15:09:19.726729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.eval()","metadata":{"execution":{"iopub.status.busy":"2021-10-10T15:09:58.048352Z","iopub.execute_input":"2021-10-10T15:09:58.048695Z","iopub.status.idle":"2021-10-10T15:09:58.057784Z","shell.execute_reply.started":"2021-10-10T15:09:58.048644Z","shell.execute_reply":"2021-10-10T15:09:58.056769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2021-10-10T15:10:42.24022Z","iopub.execute_input":"2021-10-10T15:10:42.240599Z","iopub.status.idle":"2021-10-10T15:10:42.24504Z","shell.execute_reply.started":"2021-10-10T15:10:42.240542Z","shell.execute_reply":"2021-10-10T15:10:42.243917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.eval()\ncpu_device = torch.device(\"cpu\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample.shape","metadata":{"execution":{"iopub.status.busy":"2021-10-10T15:11:20.775878Z","iopub.execute_input":"2021-10-10T15:11:20.776202Z","iopub.status.idle":"2021-10-10T15:11:20.781472Z","shell.execute_reply.started":"2021-10-10T15:11:20.776154Z","shell.execute_reply":"2021-10-10T15:11:20.780227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"outputs = model(images)\noutputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]","metadata":{"execution":{"iopub.status.busy":"2021-10-10T15:10:51.669589Z","iopub.execute_input":"2021-10-10T15:10:51.669931Z","iopub.status.idle":"2021-10-10T15:10:51.728916Z","shell.execute_reply.started":"2021-10-10T15:10:51.669876Z","shell.execute_reply":"2021-10-10T15:10:51.726234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"box = outputs[0][\"boxes\"][0].detach().numpy().astype(np.int32).tolist()\nfig, ax = plt.subplots(1, 1, figsize=(16, 8))\n\ncv2.rectangle(sample,\n              (box[0], box[1]),\n              (box[2], box[3]),\n              (220, 0, 0), 3)\n    \nax.set_axis_off()\nax.imshow(sample)","metadata":{"execution":{"iopub.status.busy":"2021-10-10T15:09:03.052836Z","iopub.execute_input":"2021-10-10T15:09:03.053183Z","iopub.status.idle":"2021-10-10T15:09:03.310599Z","shell.execute_reply.started":"2021-10-10T15:09:03.053126Z","shell.execute_reply":"2021-10-10T15:09:03.309492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n\nfor box in boxes:\n    cv2.rectangle(sample,\n                  (box[0], box[1]),\n                  (box[2], box[3]),\n                  (220, 0, 0), 3)\n    \nax.set_axis_off()\nax.imshow(sample)","metadata":{"execution":{"iopub.status.busy":"2021-10-10T12:41:30.050399Z","iopub.execute_input":"2021-10-10T12:41:30.050753Z","iopub.status.idle":"2021-10-10T12:41:30.274827Z","shell.execute_reply.started":"2021-10-10T12:41:30.050705Z","shell.execute_reply":"2021-10-10T12:41:30.273748Z"},"trusted":true},"execution_count":null,"outputs":[]}]}